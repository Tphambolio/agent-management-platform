# Agent Testing Guide
## How Agents Execute and Learn from Tests

This guide explains how agents in the wildfire simulator ecosystem can leverage the pytest-runner skill for automated quality assurance.

---

## Overview

Agents can now execute comprehensive test suites, analyze results, and update their DNA based on testing outcomes. This creates a self-improving quality assurance loop.

## Quick Start for Agents

### QA Testing Agent

**Primary Skills**: `pytest-runner`, `polygon-fire-prediction`

**Quick Validation**:
```bash
python skills/pytest-runner/scripts/run_tests.py --quick
```

**Generate Agent Report**:
```bash
python skills/pytest-runner/scripts/run_tests.py --agent-report
```

Result written to: `.agents/logs/test_agent_report.json`

---

## Test Execution Modes

### 1. Quick Mode (Before Every Commit)
```bash
python skills/pytest-runner/scripts/run_tests.py --quick
```

- **Duration**: ~0.3 seconds
- **Tests**: 21 unit tests
- **Coverage**: Core functionality
- **Use When**: Quick validation, before committing code

### 2. Security Mode (Critical)
```bash
python skills/pytest-runner/scripts/run_tests.py --security
```

- **Duration**: <1 second
- **Tests**: 4 security tests
- **Coverage**: Path traversal, input sanitization, validation
- **Use When**: Before deployment, after security-related changes
- **Critical**: Must pass 100%

### 3. Coverage Mode (Quality Gate)
```bash
python skills/pytest-runner/scripts/run_tests.py --coverage --html
```

- **Duration**: ~2 seconds
- **Tests**: All unit tests + coverage analysis
- **Coverage**: Measures code coverage %
- **Use When**: Before major releases, quality gates

### 4. Integration Mode (External APIs)
```bash
python skills/pytest-runner/scripts/run_tests.py --integration
```

- **Duration**: 30-60 seconds
- **Tests**: 6 integration tests
- **Coverage**: LANDFIRE, OpenTopography, Open-Meteo APIs
- **Use When**: Validating external integrations
- **Note**: May skip if APIs unavailable

### 5. E2E Mode (Full Workflow)
```bash
python skills/pytest-runner/scripts/run_tests.py --e2e
```

- **Duration**: 2-3 minutes
- **Tests**: 6 end-to-end tests
- **Coverage**: Complete polygon â†’ prediction â†’ results workflow
- **Use When**: Pre-deployment validation
- **Requires**: API server running

### 6. CI Mode (Automated Pipelines)
```bash
python skills/pytest-runner/scripts/run_tests.py --ci
```

- **Duration**: <5 minutes
- **Tests**: All tests + coverage + JSON report
- **Coverage**: Complete system validation
- **Use When**: CI/CD pipelines, automated quality gates
- **Outputs**: test_report.json, coverage.json, test_summary.txt

---

## Agent DNA Integration

### Updating QA Agent Genome

The QA Testing Agent's genome has been enhanced with pytest-runner capabilities:

```json
{
  "skills": {
    "technical": {
      "pytest-runner": {
        "version": "1.0.0",
        "proficiency": "expert",
        "capabilities": [
          "run_unit_tests",
          "run_integration_tests",
          "run_e2e_tests",
          "generate_coverage_reports",
          "security_validation",
          "performance_benchmarking",
          "ci_integration"
        ]
      }
    }
  },
  "experience_bank": {
    "patterns_known": [
      "test_pyramid_architecture",
      "unit_integration_e2e_separation",
      "security_first_testing"
    ],
    "techniques_mastered": [
      "pytest_markers_for_categorization",
      "fixture_based_test_data",
      "coverage_analysis"
    ]
  }
}
```

### Learning from Test Results

Agents should update their DNA after test execution:

```json
{
  "session_memory": {
    "last_session_summary": "Executed polygon API tests: 21/21 passed, coverage 85%",
    "next_priorities": [
      "Monitor integration test pass rate",
      "Investigate coverage gaps in data_fetcher module"
    ]
  },
  "evolution_metrics": {
    "tasks_completed": 1,
    "learning_velocity": "improving"
  }
}
```

---

## Task Execution Flow

### Automated Task: `validate_polygon_api`

**Location**: `.agents/tasks/validate_polygon_api.json`

**Execution Flow**:

1. **Phase 1: Quick Validation** (Required)
   - Run unit tests
   - Must pass 100%
   - Blocks deployment if failed

2. **Phase 2: Security Validation** (Required)
   - Run security tests
   - Must pass 100%
   - Critical - no exceptions

3. **Phase 3: Integration Tests** (Optional)
   - Test external APIs
   - May skip if APIs unavailable
   - 80% pass rate acceptable

4. **Phase 4: Coverage Analysis** (Required)
   - Ensure 85%+ coverage
   - Identify coverage gaps

**Success Criteria**:
- Phase 1: âœ… Required
- Phase 2: âœ… Required (no security failures allowed)
- Phase 3: âš ï¸ Optional (best effort)
- Phase 4: âœ… Required

**On Failure**:
- Notify: backend-developer-agent, software-architect-agent
- Block deployment: Yes
- Create issue: Yes
- Max retries: 2

---

## Agent Report Format

Tests generate agent-friendly reports at `.agents/logs/test_agent_report.json`:

```json
{
  "timestamp": "2025-11-02T15:30:00",
  "test_suite": "polygon-fire-prediction-api",
  "execution_summary": {
    "total_tests": 21,
    "passed": 21,
    "failed": 0,
    "skipped": 0,
    "duration_seconds": 0.3,
    "pass_rate": 1.0
  },
  "quality_metrics": {
    "code_coverage": 0.85,
    "security_tests": "all_passing",
    "performance": "acceptable",
    "reliability": 0.991
  },
  "recommendations": [
    "EXCELLENT: System quality validated"
  ],
  "action_required": false
}
```

### Agent Decision Logic

```python
def should_deploy(report):
    if report['execution_summary']['failed'] > 0:
        return False, "FAILED TESTS - Block deployment"

    if report['quality_metrics']['security_tests'] != 'all_passing':
        return False, "SECURITY FAILURE - Block deployment"

    if report['execution_summary']['pass_rate'] < 0.95:
        return False, "LOW PASS RATE - Investigate failures"

    if report['quality_metrics']['code_coverage'] < 0.85:
        return False, "LOW COVERAGE - Add more tests"

    return True, "DEPLOY APPROVED"
```

---

## DNA Evolution Patterns

### Pattern 1: Test-Driven Learning

```json
{
  "pattern": "test_first_development",
  "trigger": "new_feature_request",
  "action": "run_quick_tests_before_changes",
  "validation": "run_full_suite_after_changes",
  "learning": "update_coverage_gaps_in_dna"
}
```

### Pattern 2: Security-First Testing

```json
{
  "pattern": "security_gate",
  "trigger": "before_deployment",
  "action": "run_security_tests",
  "acceptance": "100% pass rate required",
  "failure_action": "block_deployment_and_escalate"
}
```

### Pattern 3: Continuous Improvement

```json
{
  "pattern": "coverage_improvement",
  "trigger": "after_each_session",
  "action": "analyze_coverage_report",
  "learning": "identify_untested_code_paths",
  "next_action": "create_tests_for_gaps"
}
```

---

## Integration with Agent Workflows

### Backend Developer Agent

**Before Committing**:
```bash
python skills/pytest-runner/scripts/run_tests.py --quick
```

**After Feature Complete**:
```bash
python skills/pytest-runner/scripts/run_tests.py --coverage
```

### Software Architect Agent

**System Health Check**:
```bash
python skills/pytest-runner/scripts/run_tests.py --all
```

**Architecture Validation**:
- Review coverage reports
- Identify untested components
- Plan test expansion

### Fire Behavior Specialist Agent

**Validation Tests**:
```bash
pytest tests/test_fbp_validation.py -v
pytest tests/validation/test_historical_fires.py -v
```

**After Algorithm Changes**:
```bash
python skills/pytest-runner/scripts/run_tests.py --unit
```

---

## Feedback Loop Integration

### 1. Execute Tests
```bash
python skills/pytest-runner/scripts/run_tests.py --agent-report
```

### 2. Read Agent Report
```python
import json
with open('.agents/logs/test_agent_report.json') as f:
    report = json.load(f)
```

### 3. Update DNA
```python
def update_agent_dna(report):
    genome['evolution_metrics']['tasks_completed'] += 1

    if report['execution_summary']['pass_rate'] >= 0.99:
        genome['evolution_metrics']['learning_velocity'] = 'improving'
        genome['session_memory']['next_priorities'].append(
            'Maintain high quality standards'
        )
    elif report['action_required']:
        genome['session_memory']['next_priorities'].append(
            f"Fix failing tests: {report['recommendations']}"
        )
```

### 4. Log to Feedback Registry
```bash
echo "{\"task\": \"validate_polygon_api\", \"result\": \"success\", \"timestamp\": \"$(date -Iseconds)\"}" >> .agents/logs/feedback_registry.json
```

---

## Best Practices for Agents

### 1. Test Before You Code
- Run quick tests before making changes
- Understand current test coverage
- Identify what needs testing

### 2. Security is Non-Negotiable
- Always run security tests before deployment
- 100% pass rate required
- No exceptions

### 3. Coverage is a Guide, Not a Goal
- Aim for 85%+ coverage
- Focus on critical paths
- Don't chase 100% mindlessly

### 4. Learn from Failures
- Update DNA when tests fail
- Document patterns
- Avoid repeat mistakes

### 5. Integrate with CI/CD
- Automate test execution
- Block bad deployments
- Generate reports for humans

---

## Example Agent Session

```bash
# 1. Agent starts task
echo "Starting validate_polygon_api task..."

# 2. Quick validation
python skills/pytest-runner/scripts/run_tests.py --quick
# âœ… 21/21 passed

# 3. Security check (critical)
python skills/pytest-runner/scripts/run_tests.py --security
# âœ… 4/4 passed

# 4. Coverage analysis
python skills/pytest-runner/scripts/run_tests.py --coverage --html
# âœ… Coverage: 85.3%

# 5. Generate agent report
python skills/pytest-runner/scripts/run_tests.py --agent-report
# âœ… Report: .agents/logs/test_agent_report.json

# 6. Update DNA
# - tasks_completed: 1
# - learning_velocity: improving
# - next_priorities: Monitor coverage gaps

# 7. Log to feedback registry
# Result: success, quality validated

# 8. Decision: DEPLOY APPROVED âœ…
```

---

## Troubleshooting

### Tests Failing

**Agent Action**:
1. Read failure details from report
2. Identify which test failed
3. Update DNA with failure pattern
4. Notify relevant agent (backend-dev, architect)
5. Block deployment

### Coverage Below 85%

**Agent Action**:
1. Generate HTML coverage report
2. Identify untested code paths
3. Create task to add tests
4. Update DNA priorities

### External APIs Unavailable

**Agent Action**:
1. Integration tests will skip
2. Log warning in agent report
3. Don't block deployment
4. Retry later

---

## Success Metrics

Agents should track these metrics:

```json
{
  "testing_metrics": {
    "total_test_runs": 42,
    "average_pass_rate": 0.994,
    "security_tests_never_failed": true,
    "average_coverage": 0.872,
    "tests_added_by_agent": 5,
    "bugs_caught_before_deployment": 12
  }
}
```

---

## Next Steps for Agents

1. âœ… Skill acquired: pytest-runner
2. âœ… DNA updated with testing capabilities
3. âœ… Task created: validate_polygon_api
4. ðŸ”„ **Execute first test run**
5. ðŸ”„ **Analyze results and update DNA**
6. ðŸ”„ **Integrate into deployment workflow**
7. ðŸ”„ **Establish testing patterns**

---

## Summary

Agents can now:
- âœ… Execute comprehensive test suites
- âœ… Analyze test results automatically
- âœ… Update their DNA based on outcomes
- âœ… Generate agent-friendly reports
- âœ… Integrate testing into workflows
- âœ… Block bad deployments
- âœ… Learn from failures
- âœ… Improve continuously

**Cost**: $0 (vs AI vision testing)
**Speed**: <5 minutes (full suite)
**Reliability**: 99.1% pass rate
**Integration**: Seamless with agent ecosystem

---

**Test early. Test often. Learn always.** ðŸ¤–âœ…
